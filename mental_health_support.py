# -*- coding: utf-8 -*-
"""Mental Health Support.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hcbNd04F3yWgCWcTQTgEw_Eo5Ad9-A0n
"""

!pip install langchain_groq langchain_core

from langchain_groq import ChatGroq
llm = ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model = "llama-3.3-70b-versatile"
)
result = llm.invoke("Who is Lord Ram?")
print(result.content)

!pip install pypdf

!pip install chromadb

!pip install -U langchain-community

import os
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import PyPDFLoader,DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter

def initialize_llm():
  llm = ChatGroq(
    temperature = 0,
    groq_api_key = "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",
    model = "llama-3.3-70b-versatile"
)
  return llm

def create_vector_db():
  loader = DirectoryLoader('/content/data',glob="*.pdf",loader_cls=PyPDFLoader)
  documents = loader.load()
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
  texts = text_splitter.split_documents(documents)
  embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
  vector_db = Chroma.from_documents(texts, embeddings,persist_directory='./chroma_db')
  vector_db.persist()

  print("Chroma created and data saved")

  return vector_db

def setup_qa_chain(vector_db, llm):
  retriever = vector_db.as_retriever()
  prompt = """ You are a compassionate mental health mental health chatbot.Respond thoughtfully to the following question:
  {context}
  User: {question}
  Chatbot: """
  PROMPT = PromptTemplate(template=prompt, input_variables=["context", "question"])
  qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True, chain_type_kwargs={"prompt": PROMPT})
  return qa_chain

def main():
    print("Initializing Chatbot.....")
    llm = initialize_llm()

    db_path = "/content/chroma_db"

    if not os.path.exists(db_path):
        vector_db = create_vector_db()
    else:
        embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
        vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)

    qa_chain = setup_qa_chain(vector_db, llm)
    while True:
        query = input("\nHuman")
        if query.lower() == "exit":
            print("Chatbot: Take care of yourself, Goodbye")
            break
        # Use qa_chain.run to get just the answer, or qa_chain({"query": query}) to get both the answer and source documents
        # response = qa_chain.run(query)  # This line caused the error
        response = qa_chain({"query": query}) # calling using the call method
        print(f"Chatbot: {response['result']}") # Accessing the 'result' key to display the response

if __name__ == "__main__":
    main()

from warnings import filterwarnings
filterwarnings('ignore')
create_vector_db()

!pip install gradio

from langchain_groq import ChatGroq

# Install necessary libraries
!pip install langchain_groq langchain_core
!pip install pypdf
!pip install chromadb
!pip install -U langchain-community

import os
import gradio as gr
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.document_loaders import PyPDFLoader, DirectoryLoader
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_groq import ChatGroq

# Initialize the LLM
def initialize_llm():
    llm = ChatGroq(
        temperature=0,
        groq_api_key="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",
        model="llama3-70b-8192"
    )
    return llm

# Create the vector database
def create_vector_db():
    loader = DirectoryLoader('/content/data', glob="*.pdf", loader_cls=PyPDFLoader)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = text_splitter.split_documents(documents)
    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma.from_documents(texts, embeddings, persist_directory='./chroma_db')
    vector_db.persist()
    print("Chroma created and data saved")
    return vector_db

# Set up the QA chain
def setup_qa_chain(vector_db, llm):
    retriever = vector_db.as_retriever()
    prompt = """You are a compassionate mental health chatbot. Respond thoughtfully to the following question:
{context}
User: {question}
Chatbot:"""
    PROMPT = PromptTemplate(template=prompt, input_variables=["context", "question"])
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": PROMPT}
    )
    return qa_chain

# Initialize everything
print("Initializing Chatbot.....")
llm = initialize_llm()

db_path = "/content/chroma_db"

if not os.path.exists(db_path):
    vector_db = create_vector_db()
else:
    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
    vector_db = Chroma(persist_directory=db_path, embedding_function=embeddings)

qa_chain = setup_qa_chain(vector_db, llm)

# Chatbot response logic
def chatbot_response(message, history):
    if not message.strip():
        return "Please provide a valid input"

    try:
        response = qa_chain.invoke({"query": message})
        answer = response['result']  # Only use the 'result' part
        return answer
    except Exception as e:
        return f"Error: {str(e)}"

# Create the Gradio ChatInterface
with gr.Blocks(theme='Respair/Shiki@1.2.1') as app:
    gr.ChatInterface(
        fn=chatbot_response,
        title="Mental Health Chatbot"
    )

app.launch()