# -*- coding: utf-8 -*-
"""DL_Proj_ETE3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xqly496J7RbSFUDON-EAxs-TakJrO1Mm
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
from PIL import Image
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import layers, models

# Updated path configuration
base_path = "/content/drive/MyDrive/İNME VERİ SETİ/YarısmaVeriSeti_1_Oturum"
png_path = os.path.join(base_path, "PNG")
masks_path = os.path.join(base_path, "MASKS")
overlay_path = os.path.join(base_path, "OVERLAY")

# Verify paths
print("PNG path exists:", os.path.exists(png_path))
print("Masks path exists:", os.path.exists(masks_path))
print("Overlay path exists:", os.path.exists(overlay_path))

# Visualization code
png_files = sorted(os.listdir(png_path))[:5]
mask_files = sorted(os.listdir(masks_path))[:5]
overlay_files = sorted(os.listdir(overlay_path))[:5]

!pip install opencv-python
import cv2 # Import the OpenCV (cv2) module

fig, axes = plt.subplots(5, 3, figsize=(12, 15))
for i in range(5):
    img = cv2.imread(os.path.join(png_path, png_files[i]))
    mask = cv2.imread(os.path.join(masks_path, mask_files[i]), cv2.IMREAD_GRAYSCALE)
    overlay = cv2.imread(os.path.join(overlay_path, overlay_files[i]))

    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    overlay = cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)

    axes[i, 0].imshow(img)
    axes[i, 0].set_title("Original Image")
    axes[i, 0].axis("off")

    axes[i, 1].imshow(mask, cmap="gray")
    axes[i, 1].set_title("Mask")
    axes[i, 1].axis("off")

    axes[i, 2].imshow(overlay)
    axes[i, 2].set_title("Overlay")
    axes[i, 2].axis("off")

plt.tight_layout()
plt.show()

def load_images_and_masks(image_folder, mask_folder, image_size=(256, 256)):
    images = []
    masks = []

    image_files = sorted(os.listdir(image_folder))
    mask_files = sorted(os.listdir(mask_folder))

    for img_file, mask_file in zip(image_files, mask_files):
        img_path = os.path.join(image_folder, img_file)
        mask_path = os.path.join(mask_folder, mask_file)

        img = Image.open(img_path).convert("L").resize(image_size)
        mask = Image.open(mask_path).convert("L").resize(image_size)

        img = np.array(img) / 255.0
        mask = np.array(mask) / 255.0

        images.append(np.expand_dims(img, axis=-1))
        masks.append(np.expand_dims(mask, axis=-1))

    return np.array(images), np.array(masks)

# Load data
images, masks = load_images_and_masks(png_path, masks_path)

# GPU configuration
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print("GPUs Available:", gpus)
    except RuntimeError as e:
        print(e)
else:
    print("No GPU found. Running on CPU.")

# Split data
X_train, X_val, y_train, y_val = train_test_split(images, masks, test_size=0.2, random_state=42)

def multihead_attention_block(inputs, key_dim, num_heads):
    """Adds a multi-head attention block"""
    attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(inputs, inputs)
    attention = layers.LayerNormalization(epsilon=1e-6)(attention + inputs)
    return attention

def unet_model(input_size=(256, 256, 1)):
    inputs = tf.keras.Input(input_size)

    # Encoder
    conv1 = layers.Conv2D(64, 3, activation="relu", padding="same")(inputs)
    conv1 = layers.Conv2D(64, 3, activation="relu", padding="same")(conv1)
    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = layers.Conv2D(128, 3, activation="relu", padding="same")(pool1)
    conv2 = layers.Conv2D(128, 3, activation="relu", padding="same")(conv2)
    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)

    # Bridge with attention
    conv3 = layers.Conv2D(256, 3, activation="relu", padding="same")(pool2)
    conv3 = layers.Conv2D(256, 3, activation="relu", padding="same")(conv3)

    # Attention mechanism
    reshaped_conv3 = layers.Reshape((-1, 256))(conv3)
    attention_conv3 = multihead_attention_block(reshaped_conv3, key_dim=256//4, num_heads=4)
    attention_conv3 = layers.Reshape((conv3.shape[1], conv3.shape[2], 256))(attention_conv3)
     # Decoder
    up1 = layers.UpSampling2D(size=(2, 2))(attention_conv3)
    concat1 = layers.concatenate([conv2, up1], axis=-1)
    conv4 = layers.Conv2D(128, 3, activation="relu", padding="same")(concat1)
    conv4 = layers.Conv2D(128, 3, activation="relu", padding="same")(conv4)

    up2 = layers.UpSampling2D(size=(2, 2))(conv4)
    concat2 = layers.concatenate([conv1, up2], axis=-1)
    conv5 = layers.Conv2D(64, 3, activation="relu", padding="same")(concat2)
    conv5 = layers.Conv2D(64, 3, activation="relu", padding="same")(conv5)

    outputs = layers.Conv2D(1, 1, activation="sigmoid")(conv5)

    return tf.keras.Model(inputs, outputs)

# Create and compile model
model = unet_model(input_size=(256, 256, 1))
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
model.summary()

# Training
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    batch_size=8,
    epochs=5,
    verbose=1
)

# Plot training history
def plot_training_history(history):
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(history.history["loss"], label="Training Loss")
    plt.plot(history.history["val_loss"], label="Validation Loss")
    plt.title("Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history["accuracy"], label="Training Accuracy")
    plt.plot(history.history["val_accuracy"], label="Validation Accuracy")
    plt.title("Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend()

    plt.show()

plot_training_history(history)

!pip install seaborn
import seaborn as sns # Import seaborn library

# Enhanced Data Visualization
def plot_data_distribution():
    plt.figure(figsize=(18, 12))

    # Class distribution
    plt.subplot(2, 3, 1)
    class_counts = {
        'Normal Tissue': np.sum([np.mean(mask == 0) for mask in masks]),
        'Stroke Region': np.sum([np.mean(mask == 1) for mask in masks])
    }
    sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values())) # Use sns.barplot after importing seaborn
    plt.title("Class Distribution in Masks")
    plt.ylabel("Pixel Count (Millions)")
    plt.yscale('log')

    # Image intensity distribution
    plt.subplot(2, 3, 2)
    sample_image = np.concatenate([img.flatten() for img in images])
    sns.histplot(sample_image, bins=50, kde=True)
    plt.title("Image Intensity Distribution")

    # Mask intensity distribution
    plt.subplot(2, 3, 3)
    sample_mask = np.concatenate([mask.flatten() for mask in masks])
    sns.histplot(sample_mask, bins=50, kde=True)
    plt.title("Mask Intensity Distribution")
      # Sample overlay visualization
    plt.subplot(2, 3, 4)
    idx = np.random.randint(len(images))
    plt.imshow(images[idx].squeeze(), cmap='gray')
    plt.title("Original Image")
    plt.axis('off')

    plt.subplot(2, 3, 5)
    plt.imshow(masks[idx].squeeze(), cmap='gray')
    plt.title("Ground Truth Mask")
    plt.axis('off')

    plt.subplot(2, 3, 6)
    overlay = cv2.addWeighted(images[idx].squeeze(), 0.7, masks[idx].squeeze(), 0.3, 0)
    plt.imshow(overlay, cmap='jet')
    plt.title("Overlay Visualization")
    plt.axis('off')

    plt.tight_layout()
    plt.show()

plot_data_distribution()

# Enhanced Model Complexity Analysis
def analyze_model_complexity(model):
    print("\n\033[1m=== Model Complexity Analysis ===\033[0m")

    # Calculate FLOPs
    try:
        from tensorflow.python.profiler.model_analyzer import profile
        from tensorflow.python.profiler.option_builder import ProfileOptionBuilder
        flops = profile(tf.profiler.experimental.ProfilerOptions(),
                        options=ProfileOptionBuilder.float_operation())
        print(f"Estimated FLOPs: {flops.total_float_ops / 1e9:.2f} GFLOPs")
    except Exception as e:
        print("FLOPs calculation failed:", e)

    # Parameter breakdown
    trainable_params = np.sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])
    non_trainable_params = np.sum([tf.keras.backend.count_params(w) for w in model.non_trainable_weights])
    print(f"Trainable Parameters: {trainable_params:,}")
    print(f"Non-trainable Parameters: {non_trainable_params:,}")

    # Layer-wise complexity
    print("\nLayer-wise Parameter Distribution:")
    for layer in model.layers:
        params = layer.count_params()
        if params > 0:
            print(f"{layer.name:20} {params:>12,} ({params/trainable_params*100:.1f}%)")

class TrainingVisualizer(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        if epoch % 2 == 0:
            self.plot_predictions(epoch) # Pass epoch to plot_predictions

    def plot_predictions(self, epoch): # Accept epoch as argument
        sample_idx = np.random.randint(len(X_val))
        prediction = self.model.predict(X_val[sample_idx][np.newaxis, ...])

        plt.figure(figsize=(15, 5))
        plt.subplot(1, 3, 1)
        plt.imshow(X_val[sample_idx].squeeze(), cmap='gray')
        plt.title("Input Image")

        plt.subplot(1, 3, 2)
        plt.imshow(y_val[sample_idx].squeeze(), cmap='gray')
        plt.title("Ground Truth")

        plt.subplot(1, 3, 3)
        plt.imshow(prediction.squeeze() > 0.5, cmap='gray')
        plt.title("Model Prediction")

        plt.suptitle(f"Epoch {epoch} Prediction Example") # Use the passed epoch value
        plt.show()
plot_data_distribution()

!pip install gradio --quiet

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Set paths to dataset directories
import os

base_path = '/content/drive/MyDrive/Dataset'  # Adjust if your folder is nested deeper
ct_path = os.path.join(base_path, 'Brain Tumor CT scan')
mri_path = os.path.join(base_path, 'Brain Tumor MRI images')

print("CT Dataset Path:", ct_path)
print("MRI Dataset Path:", mri_path)

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Augmentation
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    rotation_range=20,
    zoom_range=0.2,
    horizontal_flip=True
)

# Training and validation data generators for MRI
# Define img_height and img_width
img_height, img_width = 224, 224 # Set your desired image size

# Define batch_size here
batch_size = 32  # You can adjust this value

train_mri = datagen.flow_from_directory(
    mri_path,
    target_size=(img_height, img_width),
    batch_size=batch_size, # Now batch_size is defined
    class_mode='binary',
    subset='training'
)

val_mri = datagen.flow_from_directory(
    mri_path,
    target_size=(img_height, img_width),
    batch_size=batch_size, # Now batch_size is defined
    class_mode='binary',
    subset='validation'
)

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import os

# Build filepaths and labels
categories = ['Tumor', 'Healthy']
image_paths = []
labels = []

for category in categories:
    category_path = os.path.join(mri_path, category)
    for image_name in os.listdir(category_path):
        if image_name.lower().endswith(('jpg', 'jpeg', 'png')):
            image_paths.append(os.path.join(category_path, image_name))
            labels.append(category)

# Create dataframe
df = pd.DataFrame({'image_path': image_paths, 'label': labels})

# Plot bar chart
plt.figure(figsize=(8, 6))
sns.countplot(data=df, x='label', palette='viridis')
plt.title("Distribution of Tumor Types", fontsize=14, fontweight='bold')
plt.xlabel("Tumor Type")
plt.ylabel("Count")

for p in plt.gca().patches:
    plt.text(p.get_x() + p.get_width()/2., p.get_height(), f'{int(p.get_height())}',
             ha='center', va='bottom', fontsize=11)

plt.show()

label_counts = df['label'].value_counts()
colors = sns.color_palette("viridis", len(label_counts))

plt.figure(figsize=(8, 6))
plt.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', startangle=140,
        colors=colors, textprops={'fontsize': 12, 'weight': 'bold'},
        wedgeprops={'edgecolor': 'black', 'linewidth': 1})
plt.title("Distribution of Tumor Types - Pie Chart", fontsize=14, fontweight='bold')
plt.show()

import cv2

def plot_sample_images(df, categories, num_images=5):
    plt.figure(figsize=(15, 8))
    for i, category in enumerate(categories):
        images = df[df['label'] == category]['image_path'].iloc[:num_images]
        for j, img_path in enumerate(images):
            img = cv2.imread(img_path)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            plt.subplot(len(categories), num_images, i * num_images + j + 1)
            plt.imshow(img)
            plt.title(category)
            plt.axis('off')
    plt.tight_layout()
    plt.show()

plot_sample_images(df, categories)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# Step 5: CNN Model
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(img_height, img_width, 3)),
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.summary()

# Step 6: Model Training
history = model.fit(
    train_mri,
    steps_per_epoch=train_mri.samples // batch_size,
    validation_data=val_mri,
    validation_steps=val_mri.samples // batch_size,
    epochs=10
)

# Step 7: Accuracy & Loss Plots
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

from sklearn.metrics import confusion_matrix

val_mri.reset()  # Use val_mri instead of val_data
pred = model.predict(val_mri)
y_pred = (pred > 0.5).astype(int).reshape(-1)
y_true = val_mri.classes

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=val_mri.class_indices,  # Use val_mri instead of val_data
            yticklabels=val_mri.class_indices)  # Use val_mri instead of val_data
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import classification_report

# Step 8: Evaluation
val_mri.reset()
pred = model.predict(val_mri, steps=val_mri.samples // batch_size, verbose=1)
predictions = (pred > 0.5).astype(int)

print("Classification Report:")
print(classification_report(val_mri.classes[:len(predictions)], predictions))

# ... (rest of the code)

!pip install gradio --quiet

import gradio as gr
import cv2
import numpy as np

def predict_with_gradio(img):
    # Convert PIL image to NumPy array
    img = np.array(img)
    img = cv2.resize(img, (224, 224))
    img = img / 255.0
    img = np.expand_dims(img, axis=0)

    prediction = model.predict(img)[0][0]
    label = "Tumor" if prediction > 0.5 else "No Tumor"
    confidence = f"{prediction:.2f}" if prediction > 0.5 else f"{1 - prediction:.2f}"

    return f"{label} ({confidence} confidence)"

interface = gr.Interface(
    fn=predict_with_gradio,
    inputs=gr.Image(type="pil"),
    outputs="text",
    title="Brain Tumor Detector",
    description="Upload an MRI or CT scan to detect Tumor presence",
    theme="default"
)

interface.launch(share=True)  # Set share=False if you don't need a public link

